{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-14T17:13:37.472245Z",
     "start_time": "2025-07-14T17:13:36.112508Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "StyleGAN - well-developed type of GAN network, which is able to generate realistic images. It uses several specific architectural tricks, which, for example, make model to pay attention on image styles. The basic element of StyleGAN - is a style-modulation. It makes unique filters for each element of data batch. Here is the way how style-modulation block with convolution could be realised:",
   "id": "383bdc8e359b2275"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:13:37.487112Z",
     "start_time": "2025-07-14T17:13:37.481251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ð¡lassic nn.Conv2d creates the same filters for all images in a batch, but in StyleGAN we want each element in the batch to have its own unique filters, so instead of classic nn.Conv2d this block will use F.conv2d for convolution operation and random convolution weights that will be updated during training.\n",
    "\n",
    "class ModulatedCond2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, cond_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.eps = 1e-8\n",
    "\n",
    "        # here we create not a layer, but convolution WEIGHTS - weights in pytorch have the form [out_channels, in_channels, kernel_size, kernel_size]\n",
    "        # then we will update this tensor in backward. This tensor is a learnable parameter shared by the entire batch. It will be scaled (modulated) individually on each element.\n",
    "        \n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(1, out_channels, in_channels, kernel_size, kernel_size)\n",
    "        )\n",
    "        \n",
    "        # here is a layer for modulated style vector\n",
    "        self.style = nn.Linear(cond_dim, in_channels)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x - original tensor (for example of size - [B, C, H, W])\n",
    "        # y - style vector, random noise after style mapping - passing through several convolution layers\n",
    "        # (for example of size - [B, C])\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # bring the modulated style vector to a size where it can be multiplied by weight\n",
    "        style = self.style(y).view(B, 1, C, 1, 1)\n",
    "        print(f\"Style shape: {style.shape}\")\n",
    "\n",
    "        # !!! This is the modulation of weights - multiplication of the convolution weights by the style vector\n",
    "        weight = self.weight * style\n",
    "        print(f\"Weight shape 1: {weight.shape}\")\n",
    "\n",
    "        # here weight has dimension [B, 256, 512, 3, 3], and sum([2, 3, 4] \"collapses\" the last three dimensions and the tensor itself to size [B, 256]\n",
    "        \n",
    "        demod = torch.rsqrt((weight ** 2).sum([2, 3, 4]) + self.eps)  \n",
    "        print(f\"Demodulation shape: {demod.shape}\")\n",
    "        \n",
    "        # this is the process of demodulation - multiplication of weights by the demodulated vector\n",
    "        weight = weight * demod.view(B, self.out_channels, 1, 1, 1)\n",
    "        print(f\"Weight shape 2: {weight.shape}\")\n",
    "\n",
    "        # bring the input tensor x and the modulated weight to the correct size for feeding into the group convolution - this is a special mode of operation of Conv2d, in which the input and output channels are divided into groups, and each group of channels is processed separately by its own filters, without interaction with other groups.\n",
    "        \n",
    "        x = x.view(1, B * C, H, W)\n",
    "        weight = weight.view(B * self.out_channels, C, self.kernel_size, self.kernel_size)\n",
    "\n",
    "        print(f\"Weight shape 3: {weight.shape}\")\n",
    "        print(f\"X shape: {x.shape}\")\n",
    "\n",
    "        # weight will have size [512, 512, 3, 3]\n",
    "        # x will have size [1, 1024, 16, 16]\n",
    "\n",
    "        # group convolution divides x into B groups of 512 channels and weight divides into B groups of 256 channels. Each group is convolved independently!\n",
    "        \n",
    "        out = F.conv2d(x, weight, padding=self.kernel_size // 2, groups=B)\n",
    "\n",
    "        \n",
    "        out = out.view(B, self.out_channels, H, W)\n",
    "        return out"
   ],
   "id": "35ec48e217e41b3b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:13:37.619529Z",
     "start_time": "2025-07-14T17:13:37.561911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor1 = torch.randn(1, 512, 16, 16)\n",
    "tensor2 = torch.randn(1, 26)\n",
    "\n",
    "block = ModulatedCond2d(512, 256, 3, 26)\n",
    "\n",
    "out = block(tensor1, tensor2)\n",
    "out.shape"
   ],
   "id": "e316bbf1259bf297",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style shape: torch.Size([1, 1, 512, 1, 1])\n",
      "Weight shape 1: torch.Size([1, 256, 512, 3, 3])\n",
      "Demodulation shape: torch.Size([1, 256])\n",
      "Weight shape 2: torch.Size([1, 256, 512, 3, 3])\n",
      "Weight shape 3: torch.Size([256, 512, 3, 3])\n",
      "X shape: torch.Size([1, 512, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 16, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:13:37.810419Z",
     "start_time": "2025-07-14T17:13:37.762321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor1 = torch.randn(2, 512, 16, 16)\n",
    "tensor2 = torch.randn(2, 26)\n",
    "\n",
    "block = ModulatedCond2d(512, 256, 3, 26)\n",
    "\n",
    "out = block(tensor1, tensor2)\n",
    "out.shape"
   ],
   "id": "f98f8ed988ee80e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style shape: torch.Size([2, 1, 512, 1, 1])\n",
      "Weight shape 1: torch.Size([2, 256, 512, 3, 3])\n",
      "Demodulation shape: torch.Size([2, 256])\n",
      "Weight shape 2: torch.Size([2, 256, 512, 3, 3])\n",
      "Weight shape 3: torch.Size([512, 512, 3, 3])\n",
      "X shape: torch.Size([1, 1024, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 16, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
