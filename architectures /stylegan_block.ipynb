{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-14T17:13:37.472245Z",
     "start_time": "2025-07-14T17:13:36.112508Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "StyleGAN - well-developed type of GAN network, which is able to generate realistic images. It uses several specific architectural tricks, which, for example, make model to pay attention on image styles. The basic element of StyleGAN - is a style-modulation. It makes unique filters for each element of data batch. Here is the way how style-modulation block with convolution could be realised:",
   "id": "383bdc8e359b2275"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:13:37.487112Z",
     "start_time": "2025-07-14T17:13:37.481251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ð¡lassic nn.Conv2d creates the same filters for all images in a batch, but in StyleGAN we want each element in the batch to have its own unique filters, so instead of classic nn.Conv2d this block will use F.conv2d for convolution operation and random convolution weights that will be updated during training.\n",
    "\n",
    "class ModulatedCond2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, cond_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.eps = 1e-8\n",
    "\n",
    "        # here we create not a layer, but convolution WEIGHTS - weights in pytorch have the form [out_channels, in_channels, kernel_size, kernel_size]\n",
    "        # then we will update this tensor in backward. This tensor is a learnable parameter shared by the entire batch. It will be scaled (modulated) individually on each element.\n",
    "        \n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(1, out_channels, in_channels, kernel_size, kernel_size)\n",
    "        )\n",
    "        \n",
    "        # here is a layer for modulated style vector\n",
    "        self.style = nn.Linear(cond_dim, in_channels)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x - original tensor (for example of size - [B, C, H, W])\n",
    "        # y - style vector, random noise after style mapping - passing through several convolution layers\n",
    "        # (for example of size - [B, C])\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # bring the modulated style vector to a size where it can be multiplied by weight\n",
    "        style = self.style(y).view(B, 1, C, 1, 1)\n",
    "        print(f\"Style shape: {style.shape}\")\n",
    "\n",
    "        # !!! This is the modulation of weights - multiplication of the convolution weights by the style vector\n",
    "        weight = self.weight * style\n",
    "        print(f\"Weight shape 1: {weight.shape}\")\n",
    "\n",
    "        # here weight has dimension [B, 256, 512, 3, 3], and sum([2, 3, 4] \"collapses\" the last three dimensions and the tensor itself to size [B, 256]\n",
    "        \n",
    "        demod = torch.rsqrt((weight ** 2).sum([2, 3, 4]) + self.eps)  \n",
    "        print(f\"Demodulation shape: {demod.shape}\")\n",
    "        \n",
    "        # this is the process of demodulation - multiplication of weights by the demodulated vector\n",
    "        weight = weight * demod.view(B, self.out_channels, 1, 1, 1)\n",
    "        print(f\"Weight shape 2: {weight.shape}\")\n",
    "\n",
    "        # bring the input tensor x and the modulated weight to the correct size for feeding into the group convolution - this is a special mode of operation of Conv2d, in which the input and output channels are divided into groups, and each group of channels is processed separately by its own filters, without interaction with other groups.\n",
    "        \n",
    "        x = x.view(1, B * C, H, W)\n",
    "        weight = weight.view(B * self.out_channels, C, self.kernel_size, self.kernel_size)\n",
    "\n",
    "        print(f\"Weight shape 3: {weight.shape}\")\n",
    "        print(f\"X shape: {x.shape}\")\n",
    "\n",
    "        # weight will have size [512, 512, 3, 3]\n",
    "        # x will have size [1, 1024, 16, 16]\n",
    "\n",
    "        # group convolution divides x into B groups of 512 channels and weight divides into B groups of 256 channels. Each group is convolved independently!\n",
    "        \n",
    "        out = F.conv2d(x, weight, padding=self.kernel_size // 2, groups=B)\n",
    "\n",
    "        \n",
    "        out = out.view(B, self.out_channels, H, W)\n",
    "        return out"
   ],
   "id": "35ec48e217e41b3b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:13:37.619529Z",
     "start_time": "2025-07-14T17:13:37.561911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor1 = torch.randn(1, 512, 16, 16)\n",
    "tensor2 = torch.randn(1, 26)\n",
    "\n",
    "block = ModulatedCond2d(512, 256, 3, 26)\n",
    "\n",
    "out = block(tensor1, tensor2)\n",
    "out.shape"
   ],
   "id": "e316bbf1259bf297",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style shape: torch.Size([1, 1, 512, 1, 1])\n",
      "Weight shape 1: torch.Size([1, 256, 512, 3, 3])\n",
      "Demodulation shape: torch.Size([1, 256])\n",
      "Weight shape 2: torch.Size([1, 256, 512, 3, 3])\n",
      "Weight shape 3: torch.Size([256, 512, 3, 3])\n",
      "X shape: torch.Size([1, 512, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 16, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:13:37.810419Z",
     "start_time": "2025-07-14T17:13:37.762321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor1 = torch.randn(2, 512, 16, 16)\n",
    "tensor2 = torch.randn(2, 26)\n",
    "\n",
    "block = ModulatedCond2d(512, 256, 3, 26)\n",
    "\n",
    "out = block(tensor1, tensor2)\n",
    "out.shape"
   ],
   "id": "f98f8ed988ee80e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style shape: torch.Size([2, 1, 512, 1, 1])\n",
      "Weight shape 1: torch.Size([2, 256, 512, 3, 3])\n",
      "Demodulation shape: torch.Size([2, 256])\n",
      "Weight shape 2: torch.Size([2, 256, 512, 3, 3])\n",
      "Weight shape 3: torch.Size([512, 512, 3, 3])\n",
      "X shape: torch.Size([1, 1024, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 16, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another important feature of StyleGan - is Style Mapping - preprocessing of input tensor of random noise with manual weight assigning. So before fitting of random noise into a network, it should be passed through style mapping block, which uses EqualLinear blocks - custom linear block with manual weight assigning and regulation to prevent gradient explosions (weight = weight / (in_features ** 0.5):",
   "id": "f4561ba90f6e9ea0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T10:11:32.238497Z",
     "start_time": "2025-07-18T10:11:32.235438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EqualLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        weight = torch.randn(out_features, in_features)\n",
    "        weight = weight / (in_features ** 0.5)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight, self.bias)"
   ],
   "id": "5bb0eb3dba806cfd",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T10:11:35.332765Z",
     "start_time": "2025-07-18T10:11:35.328381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor = torch.randn(1, 128)\n",
    "block = EqualLinear(128, 256)\n",
    "out = block(tensor)\n",
    "out.shape\n"
   ],
   "id": "f7e57ca81d77a457",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And this way we could create StyleMap block, which will produce several style vectors:",
   "id": "e38652f90d0fa390"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T10:15:18.994382Z",
     "start_time": "2025-07-18T10:15:18.985955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class StyleMap(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, itter):\n",
    "        super().__init__()\n",
    "        \n",
    "        # add as much as you need\n",
    "        self.net = nn.Sequential(\n",
    "            EqualLinear(in_channels, hidden_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            EqualLinear(hidden_channels, hidden_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            EqualLinear(hidden_channels, hidden_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            EqualLinear(hidden_channels, hidden_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.style = nn.ModuleList([EqualLinear(hidden_channels, out_channels) for _ in range(itter)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return [style(out) for style in self.style]"
   ],
   "id": "64f6c6ba6b78acc4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T10:28:39.050690Z",
     "start_time": "2025-07-18T10:28:39.017977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# this will return 5 style vectors\n",
    "map = StyleMap(128, 256, 512, 5)\n",
    "tensor = torch.randn(1, 128)\n",
    "\n",
    "out = map(tensor)\n",
    "print(f\"Map length: {len(out)}, Vector shape: {out[0].shape}\")"
   ],
   "id": "94e8b3681ab31b81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map length: 5, Vector shape: torch.Size([1, 512])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And the most important part - during building model, pass unique style vector from StyleMap to each layer of ModulatedCond2d:",
   "id": "337b17cacf9fa90e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T11:27:35.576653Z",
     "start_time": "2025-07-18T11:27:35.561139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# also it needs different normalising methods such as Pixel normalisation\n",
    "# or spectral normalisation. Also it could be useful to realise idea of skip connection\n",
    "\n",
    "class StyleGenerator(nn.Module):\n",
    "    def __init__(self, noise_shape=256, cond_shape=16, hidden_shape=512, out_shape=512, itter=5):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.style_map = StyleMap(noise_shape+cond_shape, hidden_shape, out_shape, itter)\n",
    "        \n",
    "        self.const_input = nn.Parameter(torch.randn(1, 512, 8, 8))\n",
    "        \n",
    "        self.conv1 = ModulatedCond2d(512, 256, 3, out_shape)\n",
    "        self.conv2 = ModulatedCond2d(256, 128, 3, out_shape)\n",
    "        self.conv3 = ModulatedCond2d(128, 64, 3, out_shape)\n",
    "        self.conv4 = ModulatedCond2d(64, 32, 3, out_shape)\n",
    "        self.conv5 = ModulatedCond2d(32, 32, 3, out_shape)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(32, 3, 3, 1, 1)\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        B, C = x.shape\n",
    "        out = self.const_input.repeat(B, 1, 1, 1)\n",
    "        \n",
    "        z = torch.cat([x, y], dim=1)\n",
    "        styles = self.style_map(z)\n",
    "        \n",
    "        out = F.leaky_relu(self.upsample(self.conv1(out, styles[0])))\n",
    "        out = F.leaky_relu(self.upsample(self.conv2(out, styles[1])))\n",
    "        out = F.leaky_relu(self.upsample(self.conv3(out, styles[2])))\n",
    "        out = F.leaky_relu(self.upsample(self.conv4(out, styles[3])))\n",
    "        out = F.leaky_relu(self.upsample(self.conv5(out, styles[4])))\n",
    "        \n",
    "        # as output activation it uses tanh, so data should be normalised in range [-1, 1]\n",
    "        return torch.tanh(self.conv6(out))\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ],
   "id": "e2da30a49105cd7e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T11:27:48.806199Z",
     "start_time": "2025-07-18T11:27:48.572067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = StyleGenerator()\n",
    "# let it be a random noise:\n",
    "tensor1 = torch.randn(1, 256)\n",
    "# and let it be a condition for generation:\n",
    "tensor2 = torch.randn(1, 16)\n",
    "\n",
    "out = model(tensor1, tensor2)\n",
    "# size of output will be [1, 3, 256, 256]\n",
    "out.shape"
   ],
   "id": "ded3001cc191223a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style shape: torch.Size([1, 1, 512, 1, 1])\n",
      "Weight shape 1: torch.Size([1, 256, 512, 3, 3])\n",
      "Demodulation shape: torch.Size([1, 256])\n",
      "Weight shape 2: torch.Size([1, 256, 512, 3, 3])\n",
      "Weight shape 3: torch.Size([256, 512, 3, 3])\n",
      "X shape: torch.Size([1, 512, 8, 8])\n",
      "Style shape: torch.Size([1, 1, 256, 1, 1])\n",
      "Weight shape 1: torch.Size([1, 128, 256, 3, 3])\n",
      "Demodulation shape: torch.Size([1, 128])\n",
      "Weight shape 2: torch.Size([1, 128, 256, 3, 3])\n",
      "Weight shape 3: torch.Size([128, 256, 3, 3])\n",
      "X shape: torch.Size([1, 256, 16, 16])\n",
      "Style shape: torch.Size([1, 1, 128, 1, 1])\n",
      "Weight shape 1: torch.Size([1, 64, 128, 3, 3])\n",
      "Demodulation shape: torch.Size([1, 64])\n",
      "Weight shape 2: torch.Size([1, 64, 128, 3, 3])\n",
      "Weight shape 3: torch.Size([64, 128, 3, 3])\n",
      "X shape: torch.Size([1, 128, 32, 32])\n",
      "Style shape: torch.Size([1, 1, 64, 1, 1])\n",
      "Weight shape 1: torch.Size([1, 32, 64, 3, 3])\n",
      "Demodulation shape: torch.Size([1, 32])\n",
      "Weight shape 2: torch.Size([1, 32, 64, 3, 3])\n",
      "Weight shape 3: torch.Size([32, 64, 3, 3])\n",
      "X shape: torch.Size([1, 64, 64, 64])\n",
      "Style shape: torch.Size([1, 1, 32, 1, 1])\n",
      "Weight shape 1: torch.Size([1, 32, 32, 3, 3])\n",
      "Demodulation shape: torch.Size([1, 32])\n",
      "Weight shape 2: torch.Size([1, 32, 32, 3, 3])\n",
      "Weight shape 3: torch.Size([32, 32, 3, 3])\n",
      "X shape: torch.Size([1, 32, 128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
