{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Basic idea of squeeze-excitation network, as attention mechanism is to assign a certain value for each extracted feature channel and evaluate which of them is the most important. SE algorithm:\n",
    "\n",
    "1) Squeeze: We take feature maps (for example 32 of them with size 224x224 of each) and apply Global average pooling (GAP) to put them all into compact form: 32x1x1 to make calculation easier. (We may also apply flatten layer for this, because we need to receive a vector with size 32 as an output).\n",
    "2)  Excitation: Having vector with 32 values, we should remember, that each of them is the level of importance of each channel. Then we need to reduce amount of channels, also to make calculation easier. We let reduced vector go through relu function, which will turn negative (less important features) values into 0. Then, we put vector into basic channel size and apply sigmoid - because during resizing to basic size, there could appear some negative values, so sigmoid turns them into 0/1 range. \n",
    "3) So we've received a vector with values in range from 0 to 1. Then we take this vector, add size dimensions (1x1) to it and multiply it on basic feature map. So if we had feature map of this size - 3, 224, 224, vector with sigmoid-weighted values will have size 3, 1, 1 and will look, for example like [0.9, 0.2, 0.1], then it means, that first channels will be multiplied on 0.9 (turned stronger), second will be multiplied on 0.2 (turned weaker) and third one will be multiplied on 0.1 (also turned weaker) - this how this layer detects priority of each channel.\n"
   ],
   "id": "77b9fd1b7d59f72"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-22T13:55:53.967251Z",
     "start_time": "2024-12-22T13:55:52.399647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from symbol import or_test\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T14:47:37.076591Z",
     "start_time": "2024-12-22T14:47:37.072199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, C, r=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.C = C\n",
    "        self.r = r\n",
    "    \n",
    "        self.glob_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.linear1 = nn.Linear(C, C//r)\n",
    "        self.linear2 = nn.Linear(C//r, C)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # basic size \n",
    "        # x.shape = [N, C, H, W]\n",
    "        \n",
    "        # global adaptive pooling - to put feature map size as 1x1\n",
    "        out = self.glob_pool(x)\n",
    "        \n",
    "        # turn vector from Cx1x1 size to 1D vector\n",
    "        out = self.flatten(out)\n",
    "        # reducing amount of channels (size of vector)\n",
    "        out = self.linear1(out)\n",
    "        # pass al values through the relu() to turn negative (less important) ones into zeros\n",
    "        out = self.relu(out)\n",
    "        # bringing original vector size back\n",
    "        out = self.linear2(out)\n",
    "        # passing original-size vector through sigmoid to detect more and less important values.\n",
    "        out = self.sigmoid(out)\n",
    "        # add 1x1 size to 1D vector with more and less important feature values.\n",
    "        out = out[:, :, None, None]\n",
    "        # multiply received 3D vector of importances with original feature map\n",
    "        out = out * x\n",
    "        \n",
    "        return out\n",
    "    \n",
    "        \n",
    "        "
   ],
   "id": "55ab6ad728fd9467",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T14:47:39.205880Z",
     "start_time": "2024-12-22T14:47:39.176885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor = torch.rand(1, 128, 224, 224)\n",
    "block = SEBlock(128, 16)\n",
    "\n",
    "out = block(tensor)\n",
    "out.shape"
   ],
   "id": "d12362d7dee16df3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 224, 224])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
