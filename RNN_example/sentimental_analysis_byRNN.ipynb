{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C7Hw0Hf9qudS"
      },
      "outputs": [],
      "source": [
        "# Reccurent Neural Network, which can make sentimental analysis of some phrases\n",
        "# this task also could be done by usual dense neural network, but this example is\n",
        "# intended to show that the solution to such a problem can be carried out in different ways\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# here we're importing LSTM and GRU blocks from keras to compare them as methods for constructing RNN\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Embedding, GRU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# here we're importing tokenizator for text files\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here we opening files with positive and negative phrases as train data\n",
        "# you may find those files (positive.txt and negative.txt) in same folder\n",
        "with open('positive.txt', 'r', encoding='utf-8') as f:\n",
        "    text_pos = f.readlines()\n",
        "    text_pos[0] = text_pos[0].replace('\\ufeff', '')\n",
        "\n",
        "\n",
        "with open('negative.txt', 'r', encoding='utf-8') as f:\n",
        "    text_neg = f.readlines()\n",
        "    text_neg[0] = text_neg[0].replace('\\ufeff', '')"
      ],
      "metadata": {
        "id": "fFd4ro9Kwj87"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we combine positive and negative phrases into one collection to simplify their feeding to the neural network\n",
        "phrases = text_pos + text_neg\n",
        "count_pos = len(text_pos)\n",
        "count_neg = len(text_neg)\n",
        "total_lines = count_pos + count_neg\n",
        "print(count_pos, count_neg , total_lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUpXub9-wp6E",
        "outputId": "08ce9558-dc06-4f38-bc53-cff510310a58"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "653 401 1054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# We break the texts into individual words (the maximum number of individual words \"maxWordsCount\" is 5000).\n",
        "# Depending on your needs, you can change this parameter\n",
        "maxWordsCount = 5000\n",
        "\n",
        "# we should delate all extra symbols as '!–\"—#$%&amp;()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r«»'\n",
        "tokenizer = Tokenizer(num_words=maxWordsCount, filters='!–\"—#$%&;()*+,-./:;<=>?@[\\\\]^_`{|}~«»', lower=True, split=' ', char_level=False)\n",
        "tokenizer.fit_on_texts(phrases)"
      ],
      "metadata": {
        "id": "uBvBYQc8yt1t"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# create a “dictionary” object that will contain the word and the number of its occurrences in the text\n",
        "dictionary = list(tokenizer.word_counts.items())\n",
        "\n",
        "\n",
        "# here we display the first 10 words from the first phrase (its serial number is zero) and the number of these words\n",
        "# in the text\n",
        "print(dictionary[:10])\n",
        "print(phrases[0][:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAcZmnT2zt3f",
        "outputId": "64673e4d-7cd1-4a28-d371-d7b4a5294bba"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('i', 481), ('love', 86), ('spending', 2), ('time', 8), ('with', 52), ('my', 266), ('family', 9), ('\\n', 1053), ('the', 252), ('sun', 1)]\n",
            "I love spending time with my family.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the length for each numeric vector into which all words will be converted\n",
        "max_text_len = 10\n",
        "\n",
        "# convert the “phrases” into a “data” object, which will be a vector where the words from the “phrases”\n",
        "# have been converted into numeric values ​​in accordance with the “dictionary”\n",
        "data = tokenizer.texts_to_sequences(phrases)\n",
        "\n",
        "# data_pad - a set of vectors that were reduced to length in accordance with the \"max_text_len\" parameter\n",
        "data_pad = pad_sequences(data, maxlen=max_text_len)\n",
        "\n",
        "print(data_pad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXH6e79-PVTY",
        "outputId": "7983c3c4-ece5-417d-b9f8-c87792bb67fd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   2 ...   6 133   1]\n",
            " [  7 359  13 ...  13 264   1]\n",
            " [  0   0   2 ... 265  45   1]\n",
            " ...\n",
            " [  0   0   9 ...   3  86   1]\n",
            " [  0   0   0 ...   0   0   1]\n",
            " [  0   0   0 ...   0   0   1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training set\n",
        "X = data_pad\n",
        "\n",
        "# testing set (one-hot codding matrix where [1, 0] means positve results and [0, 1] means negative)\n",
        "Y = np.array([[1, 0]]*count_pos + [[0, 1]]*count_neg)\n",
        "print(X.shape, Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRiV0bFLRkI1",
        "outputId": "ed54a591-8ea7-4e4c-b8d9-440ec2daa91c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1054, 10) (1054, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data shuffling\n",
        "# When we are working with machine learning data, it is important to randomly shuffle the data to avoid possible training biases.\n",
        "# This helps the model generalize better to new data.\n",
        "indeces = np.random.choice(X.shape[0], size=X.shape[0], replace=False)\n",
        "X = X[indeces]\n",
        "Y = Y[indeces]"
      ],
      "metadata": {
        "id": "Qit8Jm_hSAqw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN model creating using LSTM method\n",
        "model = Sequential()\n",
        "\n",
        "# also using Embedding method at first NN layer\n",
        "model.add(Embedding(maxWordsCount, 128, input_length = max_text_len))\n",
        "#  return_sequences = True - this means that the connection of one recurrent layer to another is expected\n",
        "model.add(LSTM(128, return_sequences = True))\n",
        "model.add(LSTM(64))\n",
        "# we have 2 neuros in the last NN layer because we literaly have binary classification task\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# all parametrs output\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRnlr2_dTvjU",
        "outputId": "94f51895-f8cb-457b-d002-973d083574f7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 10, 128)           640000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 10, 128)           131584    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                49408     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 821122 (3.13 MB)\n",
            "Trainable params: 821122 (3.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN model creating using GRU method\n",
        "# it works faster than LSTM, but NN accuracy could be lower\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(maxWordsCount, 128, input_length = max_text_len))\n",
        "model.add(GRU(128, return_sequences = True))\n",
        "model.add(GRU(64))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSH5wGzamEZp",
        "outputId": "542664a3-b828-4646-9c43-982543e95707"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 10, 128)           640000    \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 10, 128)           99072     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 64)                37248     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 776450 (2.96 MB)\n",
            "Trainable params: 776450 (2.96 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', metrics = ['accuracy'], optimizer=Adam(0.0001))"
      ],
      "metadata": {
        "id": "CBdcRD92T7on"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modeling = model.fit(X, Y, batch_size=32, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZgFGHiUT_cx",
        "outputId": "37371d28-e67c-42a6-d649-3837f9b0d1ac"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "33/33 [==============================] - 5s 31ms/step - loss: 0.6776 - accuracy: 0.6091\n",
            "Epoch 2/50\n",
            "33/33 [==============================] - 1s 31ms/step - loss: 0.6465 - accuracy: 0.6195\n",
            "Epoch 3/50\n",
            "33/33 [==============================] - 1s 31ms/step - loss: 0.6017 - accuracy: 0.6376\n",
            "Epoch 4/50\n",
            "33/33 [==============================] - 1s 31ms/step - loss: 0.4818 - accuracy: 0.8083\n",
            "Epoch 5/50\n",
            "33/33 [==============================] - 1s 30ms/step - loss: 0.3011 - accuracy: 0.8824\n",
            "Epoch 6/50\n",
            "33/33 [==============================] - 2s 51ms/step - loss: 0.2387 - accuracy: 0.8928\n",
            "Epoch 7/50\n",
            "33/33 [==============================] - 1s 44ms/step - loss: 0.2211 - accuracy: 0.8966\n",
            "Epoch 8/50\n",
            "33/33 [==============================] - 1s 32ms/step - loss: 0.2131 - accuracy: 0.8985\n",
            "Epoch 9/50\n",
            "33/33 [==============================] - 1s 36ms/step - loss: 0.2128 - accuracy: 0.8994\n",
            "Epoch 10/50\n",
            "33/33 [==============================] - 1s 36ms/step - loss: 0.2071 - accuracy: 0.9013\n",
            "Epoch 11/50\n",
            "33/33 [==============================] - 1s 34ms/step - loss: 0.2023 - accuracy: 0.9023\n",
            "Epoch 12/50\n",
            "33/33 [==============================] - 1s 35ms/step - loss: 0.2000 - accuracy: 0.9023\n",
            "Epoch 13/50\n",
            "33/33 [==============================] - 1s 34ms/step - loss: 0.2015 - accuracy: 0.9032\n",
            "Epoch 14/50\n",
            "33/33 [==============================] - 1s 31ms/step - loss: 0.1947 - accuracy: 0.9032\n",
            "Epoch 15/50\n",
            "33/33 [==============================] - 1s 36ms/step - loss: 0.2011 - accuracy: 0.9032\n",
            "Epoch 16/50\n",
            "33/33 [==============================] - 2s 48ms/step - loss: 0.1967 - accuracy: 0.9042\n",
            "Epoch 17/50\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1984 - accuracy: 0.9042\n",
            "Epoch 18/50\n",
            "33/33 [==============================] - 1s 34ms/step - loss: 0.1950 - accuracy: 0.9042\n",
            "Epoch 19/50\n",
            "33/33 [==============================] - 1s 31ms/step - loss: 0.1989 - accuracy: 0.9042\n",
            "Epoch 20/50\n",
            "33/33 [==============================] - 1s 34ms/step - loss: 0.1979 - accuracy: 0.9042\n",
            "Epoch 21/50\n",
            "33/33 [==============================] - 1s 33ms/step - loss: 0.1948 - accuracy: 0.9042\n",
            "Epoch 22/50\n",
            "33/33 [==============================] - 1s 36ms/step - loss: 0.1972 - accuracy: 0.9042\n",
            "Epoch 23/50\n",
            "33/33 [==============================] - 1s 35ms/step - loss: 0.1985 - accuracy: 0.9042\n",
            "Epoch 24/50\n",
            "33/33 [==============================] - 1s 35ms/step - loss: 0.1953 - accuracy: 0.9042\n",
            "Epoch 25/50\n",
            "33/33 [==============================] - 1s 35ms/step - loss: 0.1953 - accuracy: 0.9042\n",
            "Epoch 26/50\n",
            "33/33 [==============================] - 1s 36ms/step - loss: 0.1974 - accuracy: 0.9042\n",
            "Epoch 27/50\n",
            "33/33 [==============================] - 2s 58ms/step - loss: 0.1948 - accuracy: 0.9042\n",
            "Epoch 28/50\n",
            "33/33 [==============================] - 1s 43ms/step - loss: 0.1925 - accuracy: 0.9042\n",
            "Epoch 29/50\n",
            "33/33 [==============================] - 1s 31ms/step - loss: 0.1959 - accuracy: 0.9042\n",
            "Epoch 30/50\n",
            "33/33 [==============================] - 1s 30ms/step - loss: 0.1964 - accuracy: 0.9042\n",
            "Epoch 31/50\n",
            "33/33 [==============================] - 1s 30ms/step - loss: 0.1955 - accuracy: 0.9042\n",
            "Epoch 32/50\n",
            "33/33 [==============================] - 1s 36ms/step - loss: 0.1930 - accuracy: 0.9042\n",
            "Epoch 33/50\n",
            "33/33 [==============================] - 1s 36ms/step - loss: 0.1919 - accuracy: 0.9042\n",
            "Epoch 34/50\n",
            "33/33 [==============================] - 1s 34ms/step - loss: 0.2000 - accuracy: 0.9042\n",
            "Epoch 35/50\n",
            "33/33 [==============================] - 1s 34ms/step - loss: 0.1959 - accuracy: 0.9042\n",
            "Epoch 36/50\n",
            "33/33 [==============================] - 1s 33ms/step - loss: 0.1986 - accuracy: 0.9042\n",
            "Epoch 37/50\n",
            "33/33 [==============================] - 1s 39ms/step - loss: 0.1981 - accuracy: 0.9042\n",
            "Epoch 38/50\n",
            "33/33 [==============================] - 2s 57ms/step - loss: 0.1955 - accuracy: 0.9042\n",
            "Epoch 39/50\n",
            "33/33 [==============================] - 1s 39ms/step - loss: 0.1936 - accuracy: 0.9042\n",
            "Epoch 40/50\n",
            "33/33 [==============================] - 1s 31ms/step - loss: 0.1944 - accuracy: 0.9042\n",
            "Epoch 41/50\n",
            "33/33 [==============================] - 1s 36ms/step - loss: 0.1964 - accuracy: 0.9042\n",
            "Epoch 42/50\n",
            "33/33 [==============================] - 1s 35ms/step - loss: 0.2021 - accuracy: 0.9042\n",
            "Epoch 43/50\n",
            "33/33 [==============================] - 1s 35ms/step - loss: 0.1941 - accuracy: 0.9042\n",
            "Epoch 44/50\n",
            "33/33 [==============================] - 1s 32ms/step - loss: 0.1940 - accuracy: 0.9042\n",
            "Epoch 45/50\n",
            "33/33 [==============================] - 1s 32ms/step - loss: 0.1991 - accuracy: 0.9042\n",
            "Epoch 46/50\n",
            "33/33 [==============================] - 1s 34ms/step - loss: 0.1978 - accuracy: 0.9042\n",
            "Epoch 47/50\n",
            "33/33 [==============================] - 1s 33ms/step - loss: 0.1928 - accuracy: 0.9042\n",
            "Epoch 48/50\n",
            "33/33 [==============================] - 1s 40ms/step - loss: 0.1957 - accuracy: 0.9042\n",
            "Epoch 49/50\n",
            "33/33 [==============================] - 2s 54ms/step - loss: 0.1944 - accuracy: 0.9042\n",
            "Epoch 50/50\n",
            "33/33 [==============================] - 1s 34ms/step - loss: 0.1931 - accuracy: 0.9042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this line creates a dictionary, which converts indices back to words\n",
        "index_to_word = dict(map(reversed, tokenizer.word_index.items()))\n",
        "\n",
        "# this function converts indices back to words using \"index_to_word\" dictionary\n",
        "def sequence_to_text(index_list):\n",
        "    words = [index_to_word.get(letter) for letter in index_list]\n",
        "    return(words)\n",
        "\n",
        "# we taking any example from our training data (positive.txt and negative.txt)\n",
        "t = \"The beauty of life is in the small, everyday moments.\".lower()\n",
        "# we putting our example phrase into index form\n",
        "data = tokenizer.texts_to_sequences([t])\n",
        "\n",
        "# here we checking checking lenght of test phrase and changing it if necessary\n",
        "data_pad = pad_sequences(data, maxlen=max_text_len)\n",
        "\n",
        "# we may check how our RNN handles given phrase\n",
        "# print( sequence_to_text(data[0]) )\n",
        "\n",
        "# RNN prediction based on data_pad\n",
        "res = model.predict(data_pad)\n",
        "\n",
        "# if RNN returns vector [0.1], the phrase is positive, if returned vector is closer to 1.0], phrase is negative\n",
        "# this string could output values of vector, where np.argmax(res) is index of each element in \"res\" object\n",
        "# print(res, np.argmax(res), sep='\\n')\n",
        "\n",
        "\n",
        "if np.argmax(res) == 1:\n",
        "  print(\"Phrase is pessimistic\")\n",
        "elif np.argmax(res) == 0:\n",
        "  print(\"Phrase is optimistic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XbK-fgOZbkr",
        "outputId": "7eddca29-b157-4551-dd09-baab74940175"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "Phrase is optimistic\n"
          ]
        }
      ]
    }
  ]
}